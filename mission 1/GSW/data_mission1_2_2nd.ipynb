{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "8NXsjXkX1SBk",
        "CwzWTPC-mMnx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 로컬런타임 연결"
      ],
      "metadata": {
        "id": "-rZf0qkRxe5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgAi030jnrL-",
        "outputId": "11c8f434-17ef-4b5a-dde7-9ec9b67b8761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (0.18.0+cu118)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"로컬 런타임 연결(터미널)\n",
        "activate tensorflow\n",
        "\n",
        "jupyter notebook \\\n",
        "    --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        "    --port=8888 \\\n",
        "    --NotebookApp.port_retries=0\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2WUh9FiYKBGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40ff808d-22de-4abd-e131-2c697b1eaa39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"로컬 런타임 연결(터미널)\\nactivate tensorflow\\n\\njupyter notebook     --NotebookApp.allow_origin='https://colab.research.google.com'     --port=8888     --NotebookApp.port_retries=0\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "uDid_pqeIBXd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import sys\n",
        ">>> print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1watJxMJcsFJ",
        "outputId": "292222d1-2ede-4638-ccba-08e3549442b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "print(torch.__version__) #토치 버전 '2.2.2+cu118'\n",
        "print(torch.version.cuda) #CUDA 버전\n",
        "print(torch.cuda.get_device_name())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "FO-bFwGkg1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c89a09e-8701-4b69-d79d-04061badfd49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu118\n",
            "11.8\n",
            "NVIDIA GeForce MX450\n",
            "1\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Print the installed version of TensorFlow\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# List all available devices detected by TensorFlow\n",
        "print(\"Available devices:\")\n",
        "devices = tf.config.list_physical_devices()\n",
        "for device in devices:\n",
        "    print(device)\n",
        "\n",
        "# Check if TensorFlow is currently using a GPU\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "if gpu_available:\n",
        "    print(\"GPU is available for TensorFlow.\")\n",
        "else:\n",
        "    print(\"GPU is not available, TensorFlow is using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl8gc7Hyp1Ek",
        "outputId": "3f1bd687-dbf9-4500-c176-c66d99adc5af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.10.0\n",
            "Available devices:\n",
            "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "GPU is available for TensorFlow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mission1-2."
      ],
      "metadata": {
        "id": "S_HbVRIJiBDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "xQrS-R7Acy8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69a8db7-2fee-43ab-fffa-6ecccb226b21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\SW\\.conda\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 파일 클래스별로 분류하는 코드(재실행할 필요 없음)"
      ],
      "metadata": {
        "id": "8NXsjXkX1SBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 원본 이미지 경로와 분류할 경로 설정\n",
        "source_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/training_image'\n",
        "destination_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images'\n",
        "\n",
        "# 폴더가 없다면 새로 생성\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "# 성별 폴더(W, M)를 생성\n",
        "gender_dirs = {'W': os.path.join(destination_dir, 'W'), 'M': os.path.join(destination_dir, 'M')}\n",
        "for gender_dir in gender_dirs.values():\n",
        "    if not os.path.exists(gender_dir):\n",
        "        os.makedirs(gender_dir)\n",
        "\n",
        "# os.listdir로 이미지를 리스트로 가져옴\n",
        "file_list = os.listdir(source_dir)\n",
        "\n",
        "# tqdm을 사용하여 진행 상태를 표시\n",
        "for filename in tqdm(file_list, desc=\"Sorting images\"):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # 파일명에서 성별과 클래스를 추출\n",
        "        parts = filename.split('_')\n",
        "        gender = parts[-1][0]  # 마지막 부분이 'W' 또는 'M'\n",
        "        class_name = parts[3]  # 네 번째 부분이 클래스명\n",
        "\n",
        "        # 성별에 따른 폴더 경로\n",
        "        gender_dir = gender_dirs.get(gender)\n",
        "\n",
        "        # 클래스별 폴더 경로 설정\n",
        "        class_dir = os.path.join(gender_dir, class_name)\n",
        "\n",
        "        # 클래스별 폴더가 없으면 생성\n",
        "        if not os.path.exists(class_dir):\n",
        "            os.makedirs(class_dir)\n",
        "\n",
        "        # 이미지 파일을 클래스별 폴더로 복사 (파일이 없을 때만 복사)\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "        destination_path = os.path.join(class_dir, filename)\n",
        "        if not os.path.exists(destination_path):  # 파일이 존재하지 않으면 복사\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Images have been successfully sorted into gender and class folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp-Qiva-uNlQ",
        "outputId": "d4c4bdd8-2419-4bc3-9840-bb4ff7a540b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sorting images: 100%|██████████████████████████████████████████████████████████████| 4070/4070 [00:53<00:00, 75.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been successfully sorted into gender and class folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 원본 이미지 경로와 분류할 경로 설정\n",
        "source_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/validation_image'\n",
        "destination_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_validation_images'\n",
        "\n",
        "# 폴더가 없다면 새로 생성\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "# 성별 폴더(W, M)를 생성\n",
        "gender_dirs = {'W': os.path.join(destination_dir, 'W'), 'M': os.path.join(destination_dir, 'M')}\n",
        "for gender_dir in gender_dirs.values():\n",
        "    if not os.path.exists(gender_dir):\n",
        "        os.makedirs(gender_dir)\n",
        "\n",
        "# os.listdir로 이미지를 리스트로 가져옴\n",
        "file_list = os.listdir(source_dir)\n",
        "\n",
        "# tqdm을 사용하여 진행 상태를 표시\n",
        "for filename in tqdm(file_list, desc=\"Sorting images\"):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # 파일명에서 성별과 클래스를 추출\n",
        "        parts = filename.split('_')\n",
        "        gender = parts[-1][0]  # 마지막 부분이 'W' 또는 'M'\n",
        "        class_name = parts[3]  # 네 번째 부분이 클래스명\n",
        "\n",
        "        # 성별에 따른 폴더 경로\n",
        "        gender_dir = gender_dirs.get(gender)\n",
        "\n",
        "        # 클래스별 폴더 경로 설정\n",
        "        class_dir = os.path.join(gender_dir, class_name)\n",
        "\n",
        "        # 클래스별 폴더가 없으면 생성\n",
        "        if not os.path.exists(class_dir):\n",
        "            os.makedirs(class_dir)\n",
        "\n",
        "        # 이미지 파일을 클래스별 폴더로 복사 (파일이 없을 때만 복사)\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "        destination_path = os.path.join(class_dir, filename)\n",
        "        if not os.path.exists(destination_path):  # 파일이 존재하지 않으면 복사\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Images have been successfully sorted into gender and class folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWcncGrrjHvP",
        "outputId": "b933991c-fa74-4a3b-a7f7-48aa0ecd5829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sorting images: 100%|███████████████████████████████████████████████████████████████| 951/951 [00:08<00:00, 106.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been successfully sorted into gender and class folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2차: object detection, image cropping 전처리 실시"
      ],
      "metadata": {
        "id": "pFK2Rsigyx--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W/M 성별 폴더 -> 패션 class 폴더 형식을 성별_패션 폴더 형식으로 바꾸는 코드"
      ],
      "metadata": {
        "id": "g-1N7-lRnrsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 원본 디렉토리와 목적지 디렉토리 설정\n",
        "original_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images'\n",
        "new_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images_new'\n",
        "\n",
        "# 새로운 디렉토리 생성\n",
        "if not os.path.exists(new_dir):\n",
        "    os.makedirs(new_dir)\n",
        "\n",
        "# W/M 폴더에서 이미지 파일을 가져와 새로운 클래스 폴더로 복사\n",
        "for gender in ['W', 'M']:\n",
        "    gender_dir = os.path.join(original_dir, gender)\n",
        "    for class_folder in os.listdir(gender_dir):\n",
        "        class_path = os.path.join(gender_dir, class_folder)\n",
        "        if os.path.isdir(class_path):\n",
        "            new_class_folder = f\"{gender}_{class_folder}\"  # 새로운 클래스 이름 설정\n",
        "            new_class_path = os.path.join(new_dir, new_class_folder)\n",
        "\n",
        "            # 새로운 클래스 폴더 생성\n",
        "            if not os.path.exists(new_class_path):\n",
        "                os.makedirs(new_class_path)\n",
        "\n",
        "            # 이미지 파일 복사\n",
        "            for filename in os.listdir(class_path):\n",
        "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일만 복사\n",
        "                    shutil.copy(os.path.join(class_path, filename), new_class_path)\n",
        "\n",
        "print(\"Images have been reorganized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE7T2vb6mi0R",
        "outputId": "4d9dd079-f4bb-458d-95b6-0937e8ebc431"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been reorganized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "U2-Net을 이용한 사람 이미지 추출 및 배경 제거 코드\n",
        "\n",
        "(+ MediaPipe로 먼저 중심인물의 bounding box를 계산하여, 해당 영역을 잘라냄) -> 이미지 파일이 더 이상해져서 안함"
      ],
      "metadata": {
        "id": "8FwLqWaj5Alo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from rembg import remove\n",
        "from tqdm import tqdm  # tqdm 추가\n",
        "\n",
        "# 원본 이미지 경로 및 저장할 경로 설정\n",
        "input_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images'\n",
        "output_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/processed_images'\n",
        "\n",
        "# 저장할 폴더가 없다면 생성\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# 실패한 파일명 저장을 위한 리스트\n",
        "failed_images = []\n",
        "\n",
        "# 이미지 파일 목록 생성\n",
        "image_files = []\n",
        "for root, dirs, files in os.walk(input_dir):\n",
        "    for filename in files:\n",
        "        input_file = os.path.join(root, filename)\n",
        "        if input_file.lower().endswith(('.png', '.jpg', '.jpeg')):  # 이미지 파일만 추가\n",
        "            image_files.append(input_file)\n",
        "\n",
        "# 진행 상황 표시\n",
        "for input_file in tqdm(image_files, desc=\"Processing Images\"):\n",
        "    try:\n",
        "        img = Image.open(input_file)\n",
        "        out = remove(img)\n",
        "\n",
        "        # 출력 폴더 구조 유지: 성별/W_M/클래스/파일명.png\n",
        "        relative_path = os.path.relpath(os.path.dirname(input_file), input_dir)  # 상대 경로 구하기\n",
        "        output_subdir = os.path.join(output_dir, relative_path)  # 출력 서브디렉토리 경로\n",
        "        if not os.path.exists(output_subdir):\n",
        "            os.makedirs(output_subdir)  # 서브디렉토리 생성\n",
        "\n",
        "        # 배경 제거된 이미지를 PNG 형식으로 저장\n",
        "        output_file = os.path.join(output_subdir, f\"{os.path.splitext(os.path.basename(input_file))[0]}.png\")\n",
        "        out.save(output_file, 'PNG')\n",
        "\n",
        "    except Exception as e:\n",
        "        # 에러가 발생하면 실패한 파일명을 저장\n",
        "        failed_images.append(input_file)  # 전체 경로로 저장\n",
        "        print(f\"Failed to process {input_file}: {e}\")\n",
        "\n",
        "print(\"Background removal completed.\")\n",
        "print(f\"Failed images: {len(failed_images)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "vABq9mD8ZjMl",
        "outputId": "91ae4735-4615-424b-fcfa-f3ca2bd4c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images: 100%|█████████████████████████████████████████████████████████| 4070/4070 [3:04:20<00:00,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background removal completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'count' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground removal completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount(failed_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'count' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. train, validation, test 데이터셋 분리"
      ],
      "metadata": {
        "id": "WZl7ddZerMwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
        "\n",
        "# Train 데이터셋 경로 (processed_images와 sorted_training_images)\n",
        "train_dir_1 = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/processed_images'\n",
        "train_dir_2 = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images'\n",
        "\n",
        "# Validation 데이터셋 경로\n",
        "val_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_validation_images'\n",
        "\n",
        "# 이미지 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ResNet은 224x224 입력 크기를 기대함\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet 사전 학습된 모델 정규화\n",
        "])\n",
        "\n",
        "# 각 경로의 데이터셋 불러오기\n",
        "train_dataset_1 = datasets.ImageFolder(root=train_dir_1, transform=transform)\n",
        "train_dataset_2 = datasets.ImageFolder(root=train_dir_2, transform=transform)\n",
        "\n",
        "# 두 개의 데이터셋을 합침\n",
        "train_dataset = ConcatDataset([train_dataset_1, train_dataset_2])\n",
        "\n",
        "# validation 데이터셋\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "\n",
        "\"\"\"\n",
        "# 전체 이미지 및 라벨 추출 (dataset = val_dataset)\n",
        "image_indices = np.arange(len(dataset))  # 이미지 인덱스\n",
        "labels = np.array([dataset.targets[i] for i in image_indices])  # 각 이미지의 라벨\n",
        "\n",
        "# StratifiedShuffleSplit 사용 (train 50%, validation 50%)\n",
        "splitter = StratifiedShuffleSplit(n_splits=1, test_size=476, random_state=42)\n",
        "\n",
        "val_indices, test_indices = next(splitter.split(image_indices, labels))\n",
        "\n",
        "# Subset을 사용하여 train/validation 데이터셋 분리\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\"\"\"\n",
        "\n",
        "# DataLoader 설정\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_loader.dataset)}, Val size: {len(val_loader.dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5q6CiMziM_Z",
        "outputId": "a13a2305-158c-44a9-b28f-7b5e3d50ab26"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 8140, Val size: 475, Test size: 476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 데이터셋 클래스별 이미지 수 체크"
      ],
      "metadata": {
        "id": "EMHz06C6NOMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def count_class_images_in_concat(dataset, dataset_name):\n",
        "    class_counts = Counter()\n",
        "\n",
        "    # ConcatDataset 안의 각 데이터셋을 순회\n",
        "    for sub_dataset in dataset.datasets:\n",
        "        if isinstance(sub_dataset, torch.utils.data.Subset):\n",
        "            # Subset인 경우 원본 데이터셋의 targets 사용\n",
        "            labels = [sub_dataset.dataset.targets[i] for i in sub_dataset.indices]\n",
        "        else:\n",
        "            # Subset이 아닌 경우 직접 targets 사용\n",
        "            labels = sub_dataset.targets\n",
        "\n",
        "        # 각 클래스의 이미지 수를 계산하고 합산\n",
        "        class_counts.update(labels)\n",
        "\n",
        "    # DataFrame으로 변환\n",
        "    class_distribution = pd.DataFrame(class_counts.items(), columns=['Class', 'Number of Images'])\n",
        "    class_distribution['Dataset'] = dataset_name\n",
        "\n",
        "    return class_distribution\n",
        "\n",
        "def count_class_images_in_dataset(dataset, dataset_name):\n",
        "    # Subset인 경우와 아닌 경우를 처리\n",
        "    if isinstance(dataset, torch.utils.data.Subset):\n",
        "        labels = [dataset.dataset.targets[i] for i in dataset.indices]\n",
        "    else:\n",
        "        labels = dataset.targets\n",
        "\n",
        "    # 각 클래스의 이미지 수를 계산\n",
        "    class_counts = Counter(labels)\n",
        "\n",
        "    # DataFrame으로 변환\n",
        "    class_distribution = pd.DataFrame(class_counts.items(), columns=['Class', 'Number of Images'])\n",
        "    class_distribution['Dataset'] = dataset_name\n",
        "\n",
        "    return class_distribution\n",
        "\n",
        "# Train 데이터셋이 ConcatDataset인 경우 처리\n",
        "if isinstance(train_dataset, torch.utils.data.ConcatDataset):\n",
        "    train_distribution = count_class_images_in_concat(train_dataset, \"Train\")\n",
        "else:\n",
        "    train_distribution = count_class_images_in_dataset(train_dataset, \"Train\")\n",
        "\n",
        "# Validation과 Test 데이터셋의 클래스 수 세기\n",
        "val_distribution = count_class_images_in_dataset(val_dataset, \"Validation\")\n",
        "test_distribution = count_class_images_in_dataset(test_dataset, \"Test\")\n",
        "\n",
        "# 세 DataFrame 합치기\n",
        "combined_distribution = pd.concat([train_distribution, val_distribution, test_distribution], ignore_index=True)\n",
        "\n",
        "# pivot을 사용하여 옆으로 나열\n",
        "pivot_distribution = combined_distribution.pivot(index='Class', columns='Dataset', values='Number of Images').fillna(0)\n",
        "\n",
        "# 원하는 컬럼 순서로 재배열\n",
        "pivot_distribution = pivot_distribution[['Train', 'Validation', 'Test']]\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Combined Class Distribution (Pivoted):\")\n",
        "pivot_distribution\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VDHVdkvajjeN",
        "outputId": "dfc2df53-5b99-4d66-d7e9-c41ebaf7f74e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Class Distribution (Pivoted):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset  Train  Validation  Test\n",
              "Class                           \n",
              "0          536          28    29\n",
              "1          548          33    33\n",
              "2          520          41    41\n",
              "3          474          39    40\n",
              "4          556          29    29\n",
              "5          538          40    40\n",
              "6          728          25    26\n",
              "7          596          26    26\n",
              "8          134           7     7\n",
              "9          190          11    12\n",
              "10         134           9     9\n",
              "11         154          11    11\n",
              "12          74           5     5\n",
              "13         128           9     8\n",
              "14         308          22    22\n",
              "15         154           6     6\n",
              "16          62           5     5\n",
              "17          96           4     4\n",
              "18         182           7     7\n",
              "19         182          11    11\n",
              "20         110           3     2\n",
              "21          90           4     4\n",
              "22          66           5     4\n",
              "23         278          17    18\n",
              "24         306          10    10\n",
              "25         156           9     9\n",
              "26          82           4     4\n",
              "27         240          17    17\n",
              "28         130           6     6\n",
              "29          74           8     7\n",
              "30         314          24    24"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Dataset</th>\n",
              "      <th>Train</th>\n",
              "      <th>Validation</th>\n",
              "      <th>Test</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>536</td>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>548</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>520</td>\n",
              "      <td>41</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>474</td>\n",
              "      <td>39</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>556</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>538</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>728</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>596</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>134</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>190</td>\n",
              "      <td>11</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>134</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>154</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>74</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>128</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>308</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>154</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>62</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>96</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>182</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>182</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>90</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>66</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>278</td>\n",
              "      <td>17</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>306</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>156</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>82</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>240</td>\n",
              "      <td>17</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>130</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>74</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>314</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. resnet-18 학습모델 만들기(loss와 accurancy 기록)"
      ],
      "metadata": {
        "id": "dQAGpExLrQTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ResNet-18 모델 정의\n",
        "model = models.resnet18(weights=None)  # pretrained=False로 설정\n",
        "num_features = model.fc.in_features  # 마지막 FC 레이어의 입력 크기\n",
        "model.fc = nn.Linear(num_features, 31)  # 출력 클래스 수에 맞게 수정\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss 및 Optimizer 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Learning Rate Scheduler 설정\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "# 학습 및 검증 함수\n",
        "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
        "    model.train()\n",
        "    best_accuracy = 0.0\n",
        "    history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train phase\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass 및 최적화\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Accuracy 계산\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "\n",
        "        # Train loss 및 accuracy 기록\n",
        "        history['train_loss'].append(avg_loss)\n",
        "        history['train_accuracy'].append(accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct_predictions = 0\n",
        "        val_total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "                # Accuracy 계산\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total_samples += labels.size(0)\n",
        "                val_correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        val_accuracy = val_correct_predictions / val_total_samples\n",
        "\n",
        "        # Validation loss 및 accuracy 기록\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Learning Rate 스케줄러 업데이트\n",
        "        scheduler.step()\n",
        "\n",
        "        # 가장 높은 정확도를 가진 모델 저장\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    return history\n",
        "\n",
        "# 모델 학습 및 검증\n",
        "history = train_and_validate(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktSvIYoy1xEO",
        "outputId": "385d257a-f5d5-4314-e3cb-2182ffa969a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/30: 100%|██████████████████████████████████████████████████████████▊| 254/255 [40:22<00:09,  9.65s/it]C:\\Users\\SW\\.conda\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "Training Epoch 1/30: 100%|███████████████████████████████████████████████████████████| 255/255 [40:26<00:00,  9.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Train Loss: 3.2397, Train Accuracy: 0.0876\n",
            "Validation Loss: 3.2880, Validation Accuracy: 0.0484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/30: 100%|███████████████████████████████████████████████████████████| 255/255 [40:27<00:00,  9.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/30], Train Loss: 3.2179, Train Accuracy: 0.0763\n",
            "Validation Loss: 3.2150, Validation Accuracy: 0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/30: 100%|███████████████████████████████████████████████████████████| 255/255 [40:20<00:00,  9.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/30], Train Loss: 3.2002, Train Accuracy: 0.0830\n",
            "Validation Loss: 3.2136, Validation Accuracy: 0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/30: 100%|███████████████████████████████████████████████████████████| 255/255 [40:19<00:00,  9.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/30], Train Loss: 3.1952, Train Accuracy: 0.0849\n",
            "Validation Loss: 3.2063, Validation Accuracy: 0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/30: 100%|███████████████████████████████████████████████████████████| 255/255 [40:48<00:00,  9.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30], Train Loss: 3.1914, Train Accuracy: 0.0845\n",
            "Validation Loss: 3.2083, Validation Accuracy: 0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/30: 100%|███████████████████████████████████████████████████████████| 255/255 [42:03<00:00,  9.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/30], Train Loss: 3.1876, Train Accuracy: 0.0882\n",
            "Validation Loss: 3.1896, Validation Accuracy: 0.0526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/30: 100%|███████████████████████████████████████████████████████████| 255/255 [40:13<00:00,  9.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30], Train Loss: 3.1819, Train Accuracy: 0.0878\n",
            "Validation Loss: 3.1929, Validation Accuracy: 0.0653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/30: 100%|███████████████████████████████████████████████████████████| 255/255 [41:07<00:00,  9.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/30], Train Loss: 3.1736, Train Accuracy: 0.0860\n",
            "Validation Loss: 3.1839, Validation Accuracy: 0.0653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/30: 100%|███████████████████████████████████████████████████████████| 255/255 [41:11<00:00,  9.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30], Train Loss: 3.1571, Train Accuracy: 0.0849\n",
            "Validation Loss: 3.1578, Validation Accuracy: 0.0632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10/30: 100%|██████████████████████████████████████████████████████████| 255/255 [37:39<00:00,  8.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30], Train Loss: 3.1422, Train Accuracy: 0.0888\n",
            "Validation Loss: 3.1847, Validation Accuracy: 0.0547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11/30: 100%|██████████████████████████████████████████████████████████| 255/255 [37:20<00:00,  8.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/30], Train Loss: 3.1333, Train Accuracy: 0.0908\n",
            "Validation Loss: 3.1406, Validation Accuracy: 0.0653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12/30: 100%|██████████████████████████████████████████████████████████| 255/255 [37:27<00:00,  8.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/30], Train Loss: 3.1098, Train Accuracy: 0.0980\n",
            "Validation Loss: 3.1200, Validation Accuracy: 0.0737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13/30: 100%|██████████████████████████████████████████████████████████| 255/255 [43:14<00:00, 10.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/30], Train Loss: 3.0844, Train Accuracy: 0.1000\n",
            "Validation Loss: 3.1119, Validation Accuracy: 0.0779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:34<00:00, 10.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30], Train Loss: 3.0643, Train Accuracy: 0.1138\n",
            "Validation Loss: 3.1094, Validation Accuracy: 0.0695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15/30: 100%|██████████████████████████████████████████████████████████| 255/255 [41:15<00:00,  9.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30], Train Loss: 3.0189, Train Accuracy: 0.1182\n",
            "Validation Loss: 3.0116, Validation Accuracy: 0.1411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:48<00:00, 10.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/30], Train Loss: 2.9644, Train Accuracy: 0.1286\n",
            "Validation Loss: 2.9452, Validation Accuracy: 0.1389\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17/30: 100%|██████████████████████████████████████████████████████████| 255/255 [40:48<00:00,  9.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/30], Train Loss: 2.8746, Train Accuracy: 0.1514\n",
            "Validation Loss: 2.9442, Validation Accuracy: 0.1642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18/30: 100%|██████████████████████████████████████████████████████████| 255/255 [45:09<00:00, 10.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30], Train Loss: 2.7470, Train Accuracy: 0.1770\n",
            "Validation Loss: 2.9350, Validation Accuracy: 0.1789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19/30: 100%|██████████████████████████████████████████████████████████| 255/255 [39:54<00:00,  9.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30], Train Loss: 2.5360, Train Accuracy: 0.2337\n",
            "Validation Loss: 2.6884, Validation Accuracy: 0.2674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 20/30: 100%|██████████████████████████████████████████████████████████| 255/255 [40:51<00:00,  9.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/30], Train Loss: 2.1932, Train Accuracy: 0.3333\n",
            "Validation Loss: 2.5678, Validation Accuracy: 0.3137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 21/30: 100%|██████████████████████████████████████████████████████████| 255/255 [40:04<00:00,  9.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/30], Train Loss: 1.3797, Train Accuracy: 0.5670\n",
            "Validation Loss: 2.6939, Validation Accuracy: 0.4463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 22/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:40<00:00, 10.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30], Train Loss: 1.0598, Train Accuracy: 0.6678\n",
            "Validation Loss: 2.8776, Validation Accuracy: 0.4968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 23/30: 100%|██████████████████████████████████████████████████████████| 255/255 [41:59<00:00,  9.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30], Train Loss: 0.8414, Train Accuracy: 0.7383\n",
            "Validation Loss: 3.0826, Validation Accuracy: 0.5284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 24/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:29<00:00, 10.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30], Train Loss: 0.6639, Train Accuracy: 0.7955\n",
            "Validation Loss: 3.4889, Validation Accuracy: 0.5579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 25/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:01<00:00,  9.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30], Train Loss: 0.5082, Train Accuracy: 0.8517\n",
            "Validation Loss: 4.0239, Validation Accuracy: 0.5789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 26/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:23<00:00,  9.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30], Train Loss: 0.3778, Train Accuracy: 0.8946\n",
            "Validation Loss: 4.5583, Validation Accuracy: 0.5937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 27/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:48<00:00, 10.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30], Train Loss: 0.2818, Train Accuracy: 0.9260\n",
            "Validation Loss: 5.3727, Validation Accuracy: 0.5916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 28/30: 100%|██████████████████████████████████████████████████████████| 255/255 [42:30<00:00, 10.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/30], Train Loss: 0.1995, Train Accuracy: 0.9479\n",
            "Validation Loss: 6.0294, Validation Accuracy: 0.6021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 29/30: 100%|██████████████████████████████████████████████████████████| 255/255 [40:48<00:00,  9.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/30], Train Loss: 0.1468, Train Accuracy: 0.9620\n",
            "Validation Loss: 6.6067, Validation Accuracy: 0.6000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 30/30: 100%|██████████████████████████████████████████████████████████| 255/255 [40:41<00:00,  9.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30], Train Loss: 0.1080, Train Accuracy: 0.9757\n",
            "Validation Loss: 7.4504, Validation Accuracy: 0.6021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 테스트(클래스 별 정확도 확인)"
      ],
      "metadata": {
        "id": "r6GwCwaONgej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# best 모델 로드\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()  # 평가 모드로 설정\n",
        "\n",
        "# 클래스별 라벨 이름 자동 매칭\n",
        "class_labels = train_dataset_1.classes  # ImageFolder에서 클래스 이름 가져오기\n",
        "\n",
        "# 테스트 함수 정의 (클래스별 정확도 계산 및 라벨 출력 추가)\n",
        "def evaluate(model, test_loader, criterion, num_classes, class_labels):\n",
        "    model.eval()  # 모델을 평가 모드로 전환\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # 클래스별 정확도를 계산하기 위한 변수\n",
        "    correct_per_class = [0] * num_classes  # 각 클래스별로 맞은 개수를 저장\n",
        "    total_per_class = [0] * num_classes    # 각 클래스별로 총 샘플 수를 저장\n",
        "\n",
        "    # 테스트 시에는 그래디언트가 필요 없으므로 no_grad() 사용\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # 모델에 입력 후 출력 계산\n",
        "            outputs = model(images)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # 정확도 계산\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            # 각 클래스별 맞은 개수와 총 샘플 수 추적\n",
        "            for i in range(len(labels)):\n",
        "                label = labels[i].item()\n",
        "                pred = predicted[i].item()\n",
        "                if label == pred:\n",
        "                    correct_per_class[label] += 1\n",
        "                total_per_class[label] += 1\n",
        "\n",
        "    # 평균 손실 및 전체 정확도 계산\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    overall_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # 클래스별 정확도 계산\n",
        "    class_accuracy = []\n",
        "    for i in range(num_classes):\n",
        "        if total_per_class[i] > 0:\n",
        "            accuracy = correct_per_class[i] / total_per_class[i]\n",
        "        else:\n",
        "            accuracy = 0.0\n",
        "        class_accuracy.append(accuracy)\n",
        "\n",
        "    # 결과 출력 (클래스 라벨과 함께)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
        "    for i, acc in enumerate(class_accuracy):\n",
        "        print(f\"{class_labels[i]}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "    return avg_loss, overall_accuracy, class_accuracy\n",
        "\n",
        "# 테스트셋 로더 설정 (batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 평가\n",
        "criterion = torch.nn.CrossEntropyLoss()  # 손실 함수 설정\n",
        "num_classes = 31  # 분류할 클래스 수\n",
        "test_loss, test_accuracy, class_accuracy = evaluate(model, test_loader, criterion, num_classes, class_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS-GG3DkH_Uh",
        "outputId": "b5abc9b6-3f72-4a10-e417-f8fc6709e47e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 15/15 [02:14<00:00,  8.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 6.0919, Overall Test Accuracy: 0.5903\n",
            "M_bold: Accuracy = 0.5172\n",
            "M_hiphop: Accuracy = 0.4545\n",
            "M_hippie: Accuracy = 0.6341\n",
            "M_ivy: Accuracy = 0.6750\n",
            "M_metrosexual: Accuracy = 0.6207\n",
            "M_mods: Accuracy = 0.7250\n",
            "M_normcore: Accuracy = 0.4615\n",
            "M_sportivecasual: Accuracy = 0.5385\n",
            "W_athleisure: Accuracy = 0.5714\n",
            "W_bodyconscious: Accuracy = 0.8333\n",
            "W_cityglam: Accuracy = 0.6667\n",
            "W_classic: Accuracy = 0.7273\n",
            "W_disco: Accuracy = 0.2000\n",
            "W_ecology: Accuracy = 0.2500\n",
            "W_feminine: Accuracy = 0.6818\n",
            "W_genderless: Accuracy = 0.6667\n",
            "W_grunge: Accuracy = 0.8000\n",
            "W_hiphop: Accuracy = 0.5000\n",
            "W_hippie: Accuracy = 0.5714\n",
            "W_kitsch: Accuracy = 0.4545\n",
            "W_lingerie: Accuracy = 0.5000\n",
            "W_lounge: Accuracy = 0.0000\n",
            "W_military: Accuracy = 0.2500\n",
            "W_minimal: Accuracy = 0.6667\n",
            "W_normcore: Accuracy = 0.2000\n",
            "W_oriental: Accuracy = 0.5556\n",
            "W_popart: Accuracy = 1.0000\n",
            "W_powersuit: Accuracy = 0.6471\n",
            "W_punk: Accuracy = 0.1667\n",
            "W_space: Accuracy = 0.4286\n",
            "W_sportivecasual: Accuracy = 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}