{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "-rZf0qkRxe5n",
        "8NXsjXkX1SBk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 로컬런타임 연결"
      ],
      "metadata": {
        "id": "-rZf0qkRxe5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgAi030jnrL-",
        "outputId": "11c8f434-17ef-4b5a-dde7-9ec9b67b8761"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (0.18.0+cu118)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sw\\.conda\\envs\\tensorflow\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"로컬 런타임 연결(터미널)\n",
        "activate tensorflow\n",
        "\n",
        "jupyter notebook \\\n",
        "    --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        "    --port=8888 \\\n",
        "    --NotebookApp.port_retries=0\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2WUh9FiYKBGZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40ff808d-22de-4abd-e131-2c697b1eaa39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"로컬 런타임 연결(터미널)\\nactivate tensorflow\\n\\njupyter notebook     --NotebookApp.allow_origin='https://colab.research.google.com'     --port=8888     --NotebookApp.port_retries=0\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "uDid_pqeIBXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import sys\n",
        ">>> print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1watJxMJcsFJ",
        "outputId": "ab58ca5f-0418-452e-c3e5-91b6e8dacfda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.14 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:44:50) [MSC v.1916 64 bit (AMD64)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "print(torch.__version__) #토치 버전 '2.2.2+cu118'\n",
        "print(torch.version.cuda) #CUDA 버전\n",
        "print(torch.cuda.get_device_name())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "FO-bFwGkg1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0c6c88-28ff-46e2-e19c-b0ceeaf14c72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu118\n",
            "11.8\n",
            "NVIDIA GeForce MX450\n",
            "1\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Print the installed version of TensorFlow\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# List all available devices detected by TensorFlow\n",
        "print(\"Available devices:\")\n",
        "devices = tf.config.list_physical_devices()\n",
        "for device in devices:\n",
        "    print(device)\n",
        "\n",
        "# Check if TensorFlow is currently using a GPU\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "if gpu_available:\n",
        "    print(\"GPU is available for TensorFlow.\")\n",
        "else:\n",
        "    print(\"GPU is not available, TensorFlow is using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl8gc7Hyp1Ek",
        "outputId": "93bee6bf-c2ba-4f92-9f77-f4c2a7720dc0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.10.0\n",
            "Available devices:\n",
            "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "GPU is available for TensorFlow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mission1-2."
      ],
      "metadata": {
        "id": "S_HbVRIJiBDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "from tqdm import tqdm  # Progress bar\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "xQrS-R7Acy8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fe738c-5090-4eb4-c770-1381cfd57489"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\SW\\.conda\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 파일 클래스별로 분류하는 코드(재실행할 필요 없음)"
      ],
      "metadata": {
        "id": "8NXsjXkX1SBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 원본 이미지 경로와 분류할 경로 설정\n",
        "source_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/training_image'\n",
        "destination_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images'\n",
        "\n",
        "# 폴더가 없다면 새로 생성\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "# 성별 폴더(W, M)를 생성\n",
        "gender_dirs = {'W': os.path.join(destination_dir, 'W'), 'M': os.path.join(destination_dir, 'M')}\n",
        "for gender_dir in gender_dirs.values():\n",
        "    if not os.path.exists(gender_dir):\n",
        "        os.makedirs(gender_dir)\n",
        "\n",
        "# os.listdir로 이미지를 리스트로 가져옴\n",
        "file_list = os.listdir(source_dir)\n",
        "\n",
        "# tqdm을 사용하여 진행 상태를 표시\n",
        "for filename in tqdm(file_list, desc=\"Sorting images\"):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # 파일명에서 성별과 클래스를 추출\n",
        "        parts = filename.split('_')\n",
        "        gender = parts[-1][0]  # 마지막 부분이 'W' 또는 'M'\n",
        "        class_name = parts[3]  # 네 번째 부분이 클래스명\n",
        "\n",
        "        # 성별에 따른 폴더 경로\n",
        "        gender_dir = gender_dirs.get(gender)\n",
        "\n",
        "        # 클래스별 폴더 경로 설정\n",
        "        class_dir = os.path.join(gender_dir, class_name)\n",
        "\n",
        "        # 클래스별 폴더가 없으면 생성\n",
        "        if not os.path.exists(class_dir):\n",
        "            os.makedirs(class_dir)\n",
        "\n",
        "        # 이미지 파일을 클래스별 폴더로 복사 (파일이 없을 때만 복사)\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "        destination_path = os.path.join(class_dir, filename)\n",
        "        if not os.path.exists(destination_path):  # 파일이 존재하지 않으면 복사\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Images have been successfully sorted into gender and class folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fp-Qiva-uNlQ",
        "outputId": "d4c4bdd8-2419-4bc3-9840-bb4ff7a540b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sorting images: 100%|██████████████████████████████████████████████████████████████| 4070/4070 [00:53<00:00, 75.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been successfully sorted into gender and class folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 원본 이미지 경로와 분류할 경로 설정\n",
        "source_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/validation_image'\n",
        "destination_dir = 'C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_validation_images'\n",
        "\n",
        "# 폴더가 없다면 새로 생성\n",
        "if not os.path.exists(destination_dir):\n",
        "    os.makedirs(destination_dir)\n",
        "\n",
        "# 성별 폴더(W, M)를 생성\n",
        "gender_dirs = {'W': os.path.join(destination_dir, 'W'), 'M': os.path.join(destination_dir, 'M')}\n",
        "for gender_dir in gender_dirs.values():\n",
        "    if not os.path.exists(gender_dir):\n",
        "        os.makedirs(gender_dir)\n",
        "\n",
        "# os.listdir로 이미지를 리스트로 가져옴\n",
        "file_list = os.listdir(source_dir)\n",
        "\n",
        "# tqdm을 사용하여 진행 상태를 표시\n",
        "for filename in tqdm(file_list, desc=\"Sorting images\"):\n",
        "    if filename.endswith('.jpg'):\n",
        "        # 파일명에서 성별과 클래스를 추출\n",
        "        parts = filename.split('_')\n",
        "        gender = parts[-1][0]  # 마지막 부분이 'W' 또는 'M'\n",
        "        class_name = parts[3]  # 네 번째 부분이 클래스명\n",
        "\n",
        "        # 성별에 따른 폴더 경로\n",
        "        gender_dir = gender_dirs.get(gender)\n",
        "\n",
        "        # 클래스별 폴더 경로 설정\n",
        "        class_dir = os.path.join(gender_dir, class_name)\n",
        "\n",
        "        # 클래스별 폴더가 없으면 생성\n",
        "        if not os.path.exists(class_dir):\n",
        "            os.makedirs(class_dir)\n",
        "\n",
        "        # 이미지 파일을 클래스별 폴더로 복사 (파일이 없을 때만 복사)\n",
        "        source_path = os.path.join(source_dir, filename)\n",
        "        destination_path = os.path.join(class_dir, filename)\n",
        "        if not os.path.exists(destination_path):  # 파일이 존재하지 않으면 복사\n",
        "            shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Images have been successfully sorted into gender and class folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWcncGrrjHvP",
        "outputId": "b933991c-fa74-4a3b-a7f7-48aa0ecd5829"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sorting images: 100%|███████████████████████████████████████████████████████████████| 951/951 [00:08<00:00, 106.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images have been successfully sorted into gender and class folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## resnet-18 기본모델(성별 분류) 1차"
      ],
      "metadata": {
        "id": "CwzWTPC-mMnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 학습 및 검증 함수"
      ],
      "metadata": {
        "id": "S-kLiNrsx3v2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 함수\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # 각 epoch은 train과 validation 단계로 나눠짐\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # 모델을 학습 모드로 설정\n",
        "            else:\n",
        "                model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # 데이터 로딩\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # 옵티마이저 초기화\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)  # Top-1 예측\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # 학습 단계에서만 backward + optimize 수행\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # 통계 계산\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# 테스트 함수\n",
        "def test_model(model, dataloader, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "    running_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)  # Top-1 예측\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    accuracy = running_corrects.double() / len(dataloader.dataset)\n",
        "    print(f'Test Accuracy (Top-1): {accuracy:.4f}')\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "w1hBvLFzxqDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 데이터 전처리:\n",
        "이미지를 224x224로 리사이즈하고, 일반적인 전처리 과정을 적용\n",
        "\n",
        "3. 학습 및 테스트 실행"
      ],
      "metadata": {
        "id": "AYcc_LIFx7fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "\n",
        "# 디바이스 설정 (GPU 사용 가능하면 GPU로 설정)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 전처리 (학습 및 검증용)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # 이미지를 ResNet의 입력 크기로 조정\n",
        "        transforms.RandomHorizontalFlip(),  # 데이터 증강\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ResNet에서 사용된 평균 및 표준편차로 정규화\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# 데이터셋 로드\n",
        "train_dataset = datasets.ImageFolder('C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_training_images',\n",
        "                                     transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder('C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_validation_images',\n",
        "                                   transform=data_transforms['val'])\n",
        "\n",
        "# 데이터로더 준비\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "}\n",
        "\n",
        "# 모델 정의 (간단한 ResNet-18 예시, 전이학습 미사용)\n",
        "model = models.resnet18(weights=None)  # weights=None으로 전이학습 미사용\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # 2개의 성별 클래스 (W, M)를 분류\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# 손실 함수 및 옵티마이저 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 학습 실행\n",
        "trained_model = train_model(model, dataloaders, criterion, optimizer, device, num_epochs=num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFHltuMW1JJ0",
        "outputId": "548f00d4-cd15-477f-b303-f193527ac7f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 1/10: 100%|█| 128/128 [19:53<00:00,  9.32s/batch, accuracy=tensor(0.5317, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 1/10: 100%|█| 30/30 [04:32<00:00,  9.08s/batch, accuracy=tensor(0.5605, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6777 Acc: 0.5605\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2/10: 100%|█| 128/128 [20:25<00:00,  9.57s/batch, accuracy=tensor(0.5560, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 2/10: 100%|█| 30/30 [04:35<00:00,  9.17s/batch, accuracy=tensor(0.6267, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6554 Acc: 0.6267\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3/10: 100%|█| 128/128 [21:04<00:00,  9.88s/batch, accuracy=tensor(0.5803, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 3/10: 100%|█| 30/30 [04:30<00:00,  9.02s/batch, accuracy=tensor(0.6151, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6481 Acc: 0.6151\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4/10: 100%|█| 128/128 [18:48<00:00,  8.82s/batch, accuracy=tensor(0.5749, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 4/10: 100%|█| 30/30 [04:17<00:00,  8.58s/batch, accuracy=tensor(0.6299, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6474 Acc: 0.6299\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5/10: 100%|█| 128/128 [18:49<00:00,  8.82s/batch, accuracy=tensor(0.6025, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 5/10: 100%|█| 30/30 [04:52<00:00,  9.74s/batch, accuracy=tensor(0.5731, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6705 Acc: 0.5731\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6/10: 100%|█| 128/128 [21:25<00:00, 10.05s/batch, accuracy=tensor(0.6165, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 6/10: 100%|█| 30/30 [04:55<00:00,  9.86s/batch, accuracy=tensor(0.5836, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.7164 Acc: 0.5836\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 7/10: 100%|█| 128/128 [21:42<00:00, 10.18s/batch, accuracy=tensor(0.6103, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 7/10: 100%|█| 30/30 [04:34<00:00,  9.16s/batch, accuracy=tensor(0.6309, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6368 Acc: 0.6309\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 8/10: 100%|█| 128/128 [20:49<00:00,  9.76s/batch, accuracy=tensor(0.6342, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 8/10: 100%|█| 30/30 [04:54<00:00,  9.80s/batch, accuracy=tensor(0.6698, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6198 Acc: 0.6698\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 9/10: 100%|█| 128/128 [21:16<00:00,  9.97s/batch, accuracy=tensor(0.6521, device='cuda:0', dtype=torch.floa\n",
            "Val Epoch 9/10: 100%|█| 30/30 [05:02<00:00, 10.07s/batch, accuracy=tensor(0.6488, device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.6375 Acc: 0.6488\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 10/10: 100%|█| 128/128 [20:29<00:00,  9.60s/batch, accuracy=tensor(0.6654, device='cuda:0', dtype=torch.flo\n",
            "Val Epoch 10/10: 100%|█| 30/30 [04:37<00:00,  9.26s/batch, accuracy=tensor(0.5047, device='cuda:0', dtype=torch.float64"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val Loss: 0.8223 Acc: 0.5047\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터 로드 (테스트 시에만 사용)\n",
        "test_dataset = datasets.ImageFolder('C:/Users/SW/Desktop/데크캠/2024 데이터 크리에이터 캠프 대학부/dataset/sorted_validation_images',\n",
        "                                    transform=data_transforms['test'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 테스트 실행\n",
        "test_model(trained_model, test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuIuRtm92p49",
        "outputId": "d71fe6dd-11b8-4cae-a810-6e36adaac162"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|███████████████████████████████████████████████████████| 30/30 [04:40<00:00,  9.36s/batch, accuracy=50.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 50.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-정리\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "총 4시간 소요, accuracy=50.5\n",
        "\n",
        "문제점: validation/test 파일 분리를 하지 못해서 test결과가 정확하지 못함,\n",
        "\n",
        "원본 이미지파일이 얼굴을 전부 가려서 성별 구별이 쉽지않음 + 배경이 너무 다채로움"
      ],
      "metadata": {
        "id": "T82vJNfvy6_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2차: object detection, image cropping 전처리 실시"
      ],
      "metadata": {
        "id": "pFK2Rsigyx--"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKIE52v011PZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}